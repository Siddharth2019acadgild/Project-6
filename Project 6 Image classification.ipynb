{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm \n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DownloadProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "\"\"\" \n",
    "    check if the data (zip) file is already downloaded\n",
    "    if not, download it from \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\" and save as cifar-10-python.tar.gz\n",
    "\"\"\"\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DownloadProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following steps are required for the cnn tensorflow model.\n",
    "\n",
    "#Implement Pre-process Functions\n",
    "#Normlize, One hot encoding\n",
    "#Data split in train/dev/test sets\n",
    "#Build the network\n",
    "#CNN model and its cost function & optimizer\n",
    "#Train the Neural Network\n",
    "#Hyper parameters\n",
    "#Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_names():\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "        # note the encoding type is 'latin1'\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "        \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n",
    "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
    "    \n",
    "    if not (0 <= sample_id < len(features)):\n",
    "        print('{} samples in batch {}.  {} is out of range.'.format(len(features), batch_id, sample_id))\n",
    "        return None\n",
    "\n",
    "    print('\\nStats of batch #{}:'.format(batch_id))\n",
    "    print('# of Samples: {}\\n'.format(len(features)))\n",
    "    \n",
    "    label_names = load_label_names()\n",
    "    label_counts = dict(zip(*np.unique(labels, return_counts=True)))\n",
    "    for key, value in label_counts.items():\n",
    "        print('Label Counts of [{}]({}) : {}'.format(key, label_names[key].upper(), value))\n",
    "    \n",
    "    sample_image = features[sample_id]\n",
    "    sample_label = labels[sample_id]\n",
    "    \n",
    "    print('\\nExample of Image {}:'.format(sample_id))\n",
    "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
    "    print('Image - Shape: {}'.format(sample_image.shape))\n",
    "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
    "    \n",
    "    plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch #3:\n",
      "# of Samples: 10000\n",
      "\n",
      "Label Counts of [0](AIRPLANE) : 994\n",
      "Label Counts of [1](AUTOMOBILE) : 1042\n",
      "Label Counts of [2](BIRD) : 965\n",
      "Label Counts of [3](CAT) : 997\n",
      "Label Counts of [4](DEER) : 990\n",
      "Label Counts of [5](DOG) : 1029\n",
      "Label Counts of [6](FROG) : 978\n",
      "Label Counts of [7](HORSE) : 1015\n",
      "Label Counts of [8](SHIP) : 961\n",
      "Label Counts of [9](TRUCK) : 1029\n",
      "\n",
      "Example of Image 7000:\n",
      "Image - Min Value: 24 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 0 Name: airplane\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 7000\n",
    "display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating normalzation function\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: input image data in numpy array [32, 32, 3]\n",
    "        return\n",
    "            - normalized x \n",
    "    \"\"\"\n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "    x = (x-min_val) / (max_val-min_val)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for one hot encoding\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: a list of labels\n",
    "        return\n",
    "            - one hot encoding matrix (number of labels, number of class)\n",
    "    \"\"\"\n",
    "    encoded = np.zeros((len(x), 10))\n",
    "    \n",
    "    for idx, val in enumerate(x):\n",
    "        encoded[idx][val] = 1\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
    "    n_batches = 5\n",
    "    valid_features = []\n",
    "    valid_labels = []\n",
    "\n",
    "    for batch_i in range(1, n_batches + 1):\n",
    "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
    "        \n",
    "        # find index to be the point as validation data in the whole dataset of the batch (10%)\n",
    "        index_of_validation = int(len(features) * 0.1)\n",
    "\n",
    "        # preprocess the 90% of the whole dataset of the batch\n",
    "        # - normalize the features\n",
    "        # - one_hot_encode the lables\n",
    "        # - save in a new file named, \"preprocess_batch_\" + batch_number\n",
    "        # - each file for each batch\n",
    "        _preprocess_and_save(normalize, one_hot_encode,\n",
    "                             features[:-index_of_validation], labels[:-index_of_validation], \n",
    "                             'preprocess_batch_' + str(batch_i) + '.p')\n",
    "\n",
    "        # unlike the training dataset, validation dataset will be added through all batch dataset\n",
    "        # - take 10% of the whold dataset of the batch\n",
    "        # - add them into a list of\n",
    "        #   - valid_features\n",
    "        #   - valid_labels\n",
    "        valid_features.extend(features[-index_of_validation:])\n",
    "        valid_labels.extend(labels[-index_of_validation:])\n",
    "\n",
    "    # preprocess the all stacked validation dataset\n",
    "    _preprocess_and_save(normalize, one_hot_encode,\n",
    "                         np.array(valid_features), np.array(valid_labels),\n",
    "                         'preprocess_validation.p')\n",
    "\n",
    "    # load the test dataset\n",
    "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    # preprocess the testing data\n",
    "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    test_labels = batch['labels']\n",
    "\n",
    "    # Preprocess and Save all testing data\n",
    "    _preprocess_and_save(normalize, one_hot_encode,\n",
    "                         np.array(test_features), np.array(test_labels),\n",
    "                         'preprocess_training.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model architecture\n",
    "\n",
    "#tf.nn.conv2d function can be used to build a convolutional layer which takes these inputs:\n",
    "\n",
    "# input= the output(activation) from the previous layer. This should be a 4-D tensor. Typically, in the first convolutional layer, you pass n images of size width*height*num_channels, then this has the size [n width height num_channels]\n",
    "\n",
    "# filter= trainable variables defining the filter. We start with a random normal distribution and learn these weights. It’s a 4D tensor whose specific shape is predefined as part of network design. If your filter is of size filter_size and input fed has num_input_channels and you have num_filters filters in your current layer, then filter will have following shape:\n",
    "\n",
    "\n",
    "\n",
    "#strides= defines how much you move your filter when doing convolution. In this function, it needs to be a Tensor of size>=4 i.e. [batch_stride x_stride y_stride depth_stride]. batch_stride is always 1 as you don’t want to skip images in your batch. x_stride and y_stride are same mostly and the choice is part of network design and we shall use them as 1 in our example. depth_stride is always set as 1 as you don’t skip along the depth.\n",
    "\n",
    "# padding=SAME means we shall 0 pad the input such a way that output x,y dimensions are same as that of input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flattening layer:\n",
    "\n",
    "# The Output of a convolutional layer is a multi-dimensional Tensor. We want to convert this into a one-dimensional tensor. This is done in the Flattening layer. We simply use the reshape operation to create a single dimensional tensor as defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "    conv3_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 128, 256], mean=0, stddev=0.08))\n",
    "    conv4_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 256, 512], mean=0, stddev=0.08))\n",
    "\n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "\n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
    "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "  \n",
    "    # 5, 6\n",
    "    conv3 = tf.nn.conv2d(conv2_bn, conv3_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    conv3_pool = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')  \n",
    "    conv3_bn = tf.layers.batch_normalization(conv3_pool)\n",
    "    \n",
    "    # 7, 8\n",
    "    conv4 = tf.nn.conv2d(conv3_bn, conv4_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "    conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv4_bn = tf.layers.batch_normalization(conv4_pool)\n",
    "    \n",
    "    # 9\n",
    "    flat = tf.contrib.layers.flatten(conv4_bn)  \n",
    "\n",
    "    # 10\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "    full1 = tf.nn.dropout(full1, keep_prob)\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "    \n",
    "    # 11\n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "    full2 = tf.nn.dropout(full2, keep_prob)\n",
    "    full2 = tf.layers.batch_normalization(full2)\n",
    "    \n",
    "    # 12\n",
    "    full3 = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=512, activation_fn=tf.nn.relu)\n",
    "    full3 = tf.nn.dropout(full3, keep_prob)\n",
    "    full3 = tf.layers.batch_normalization(full3)    \n",
    "    \n",
    "    # 13\n",
    "    full4 = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=1024, activation_fn=tf.nn.relu)\n",
    "    full4 = tf.nn.dropout(full4, keep_prob)\n",
    "    full4 = tf.layers.batch_normalization(full4)        \n",
    "    \n",
    "    # 14\n",
    "    out = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=10, activation_fn=None)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "keep_probability = 0.7\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-14-aa21eb445378>:11: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-aa21eb445378>:36: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-16-bc29ffa8bb57>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = conv_net(x, keep_prob)\n",
    "model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer, \n",
    "                feed_dict={\n",
    "                    x: feature_batch,\n",
    "                    y: label_batch,\n",
    "                    keep_prob: keep_probability\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    loss = sess.run(cost, \n",
    "                    feed_dict={\n",
    "                        x: feature_batch,\n",
    "                        y: label_batch,\n",
    "                        keep_prob: 1.\n",
    "                    })\n",
    "    valid_acc = sess.run(accuracy, \n",
    "                         feed_dict={\n",
    "                             x: valid_features,\n",
    "                             y: valid_labels,\n",
    "                             keep_prob: 1.\n",
    "                         })\n",
    "    \n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]\n",
    "\n",
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = pickle.load(open(filename, mode='rb'))\n",
    "\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    return batch_features_labels(features, labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3049 Validation Accuracy: 0.102000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.3056 Validation Accuracy: 0.094600\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.2927 Validation Accuracy: 0.094200\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.2984 Validation Accuracy: 0.097000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.3006 Validation Accuracy: 0.094200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.3021 Validation Accuracy: 0.102000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.3043 Validation Accuracy: 0.097000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     2.2917 Validation Accuracy: 0.094200\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     2.2991 Validation Accuracy: 0.097000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     2.2995 Validation Accuracy: 0.094200\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.3013 Validation Accuracy: 0.099800\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     2.3042 Validation Accuracy: 0.094600\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     2.2923 Validation Accuracy: 0.094200\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     2.3003 Validation Accuracy: 0.097000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     2.2999 Validation Accuracy: 0.094200\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.3017 Validation Accuracy: 0.102000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     2.3037 Validation Accuracy: 0.094600\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     2.2911 Validation Accuracy: 0.094200\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     2.2992 Validation Accuracy: 0.097000\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     2.3000 Validation Accuracy: 0.094200\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.3025 Validation Accuracy: 0.099800\n"
     ]
    }
   ],
   "source": [
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "                \n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
